{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec685b1-dfea-4b59-8946-1439a26fbce8",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- tokenization is the step of breaking down a string into atomic units used in the model\n",
    "- There're several tokenization strategies...\n",
    "- Optimal splitting of words into subunits is usuallay learned from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c07ee-ef78-429a-995f-4c435860bd68",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "\n",
    "- Simplest tokenization scheme, feeding each character individually to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495e3649-d1cf-40ad-9bc2-0722c08b049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 'k', 's', ' ', 'o', 'f', ' ', 'N', 'L', 'P']\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core taks of NLP'\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efd3a9-b700-429b-a7d3-003656483be1",
   "metadata": {},
   "source": [
    "Models expect each character to be converted to an integer, this process called *Numericalization*\n",
    "\n",
    "- One simple way to do this is by encoding each unqiue token with a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aab2da6-8db8-4ee1-b994-30be4b8e8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'L': 1, 'N': 2, 'P': 3, 'T': 4, 'a': 5, 'c': 6, 'e': 7, 'f': 8, 'g': 9, 'i': 10, 'k': 11, 'n': 12, 'o': 13, 'r': 14, 's': 15, 't': 16, 'x': 17, 'z': 18}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2708ebaa-a2e3-4bef-807e-f8f3f5454b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 13, 11, 7, 12, 10, 18, 10, 12, 9, 0, 16, 7, 17, 16, 0, 10, 15, 0, 5, 0, 6, 13, 14, 7, 0, 16, 5, 11, 15, 0, 13, 8, 0, 2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Transform tokenized_text according above mapping\n",
    "\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931ae3f-4f8e-48e6-a6c0-a0da01508e05",
   "metadata": {},
   "source": [
    "Last step is to convert input_ids to a 2D tensor of one-hot vectors.\n",
    "\n",
    "- **one hot** vectors are frequently used in ML to encode categorical data, which can be either ordinal or nomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741f753a-93c4-415a-af0a-1810fc9deee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example\n",
    "categorical_df = pd.DataFrame({'Name': ['Bumblebee', 'Optimus Prime', 'Megatron'], 'Label ID': [0, 1, 2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433463-64f2-4817-9ed4-388cbf6498ff",
   "metadata": {},
   "source": [
    "The problem of this approach is this creates an order between the names, and models can learn this kind of relationships.\n",
    "\n",
    "- Hence, we can create new columns for each category and assing 1 where the category is true, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd20482-0101-4ac3-a3d9-a5a9797cec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas we can use get_dummies()\n",
    "pd.get_dummies(categorical_df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e270f814-5304-4279-9e32-53250e6778e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1024124/2673273441.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids) # converts to a tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 19])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids) # converts to a tensor\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8238a6-0e02-4fae-a5e7-1471ec37b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 4\n",
      "One-hot: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# verifying the idea\n",
    "print(f'Token: {tokenized_text[0]}')\n",
    "print(f'Tensor index: {input_ids[0]}')\n",
    "print(f'One-hot: {one_hot_encodings[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67935a-a9bc-4ced-8962-a8a0e86fb63c",
   "metadata": {},
   "source": [
    "Character-level tokenization ignores any structure in the text an treats the whole string as stream of characters.\n",
    "- This helps to deal with misspelling and rare words\n",
    "- Drawback: requires significant compute as linguistics structurs (such as words) need to be learned from the data.\n",
    "\n",
    "Instead of this, we need to preserve some structure of the text is preserved during the tokenization step.\n",
    "- Hence Word Tokenization is being used most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a774d8-cc12-4772-8a13-862f85b3ad71",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "- Split the text into words and map each word to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a3cb19-315a-48ea-afe4-429f958ce3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'taks', 'of', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# using whitespace to tokenize the text\n",
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a519b0a-5b27-45dd-b7dd-81f5afeed9e8",
   "metadata": {},
   "source": [
    "some word tokenizers have extra rules for punctuation.\n",
    "- one can apply stemming or lemmatization, which normalize words to their stem, at the expense of losing some information of text\n",
    "- Ex: `greater`, `great` become `great`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac139f1c-6db7-4e9b-853f-3ae62491b1a5",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "\n",
    "- Combines best aspects of both character and word tokenizers\n",
    "- we want to split rare words into smaller units to allow the model to deal with complex words and misspellings.\n",
    "- Aslo we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageable size.\n",
    "- Main feature:\n",
    "    - subword tokenizer is learned from the pretraining corpus using a mix of statistical rules and algorithms.\n",
    "    - WordPiece one of algorithm that is being used in BERT and DistilBERT tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f8fc6e7-9a47-4afe-9cfc-67e24194bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: WordPiece\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4632604b-c0c6-4a6e-89af-aab7c32278a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 27006, 2015, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd1d79-9ff1-4297-93d3-6d769c0731a1",
   "metadata": {},
   "source": [
    "Here also words are mapped to unique integers. we can convert them back to tokens as following\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ea1b3b-ed52-4b92-80c4-b7d732d16426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'tak', '##s', 'of', 'nl', '##p', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773115e1-1518-43da-81ba-5c385985d85a",
   "metadata": {},
   "source": [
    "- prefix in '##izing' and '##p' means that the preceding string is not whitespace\n",
    "- Any token with this prefix should be merged with the previous token when we convert tokens back to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbbbba74-863c-4739-9a41-792f77f6b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core taks of nlp [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Converting tokens into string format\n",
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "720e44d0-5e7a-4fde-8593-28f29f4efc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size: 30522\n",
      "Model\"s maximum context size: 512\n"
     ]
    }
   ],
   "source": [
    "# Useful methods in AutoTokenizer class\n",
    "print(f'Vocab_size: {tokenizer.vocab_size}')\n",
    "print(f'Model\"s maximum context size: {tokenizer.model_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec16659-7bdf-4d65-9592-6592e311ecc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of the fields that the model expecets in the forward pass\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfdab310-681d-4cf9-aa31-bb320791dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fc201-a9b5-4021-8d2d-3c5a37e0a884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
