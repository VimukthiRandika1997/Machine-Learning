{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9df0b59-3bd9-4e88-8ff5-752d156b3378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd888c2f-6ab6-4156-8d03-2f57e2b14b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c5e95-30c1-42d7-a274-d89ac8bf6f9a",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a161447-6607-4766-8f7f-040178ddd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple PyTorch Implementation of the Graph Attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8f3c4-fcf5-4392-92fc-916e15d7732b",
   "metadata": {},
   "source": [
    "## Linear Transformation\n",
    "\n",
    "$$\n",
    "\\bar{h'}_i = \\textbf{W}\\cdot \\bar{h}_i\n",
    "$$\n",
    "with $\\textbf{W}\\in\\mathbb R^{F'\\times F}$ and $\\bar{h}_i\\in\\mathbb R^{F}$.\n",
    "\n",
    "$$\n",
    "\\bar{h'}_i \\in \\mathbb{R}^{F'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bd025f-ed37-4221-a97d-5e5206b9035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # Xavier Parameter Initialization\n",
    "# nn.init.xavier_uniform(W.data, gain=1.414)\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "input = torch.rand(nb_nodes, in_features)\n",
    "\n",
    "# Linear Transforamtion\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "print(h.shape) # Output after linear trainsformation\n",
    "print(N) # number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3397fc7-9211-49cc-8216-1f8efb410940",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdfcd4-2c09-43c9-82f8-83295c166b0d",
   "metadata": {},
   "source": [
    "![title](./Images/AttentionMechanism.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcdb51a-ae1c-4098-b46c-2690e3526ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.zeros(size=(2 * out_features, 1))) # Xavier Parameter Initialization\n",
    "nn.init.xavier_uniform_(a.data, gain=1.44)\n",
    "print(a.shape)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2) # LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b9f120-839c-49aa-a1c0-dddd394d66b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)],dim=1).view(N, -1, 2 * out_features) \n",
    "print(a_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3335e2db-756f-4b86-ab43-ccd8c99ab337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885]], grad_fn=<MmBackward0>) torch.Size([3, 2])\n",
      "========\n",
      "tensor([[-0.2666, -0.1805, -0.2666, -0.1805, -0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813,  0.9001, -0.7813,  0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885,  0.5976,  0.7885,  0.5976,  0.7885]],\n",
      "       grad_fn=<RepeatBackward0>) torch.Size([3, 6])\n",
      "========\n",
      "tensor([[-0.2666, -0.1805],\n",
      "        [-0.2666, -0.1805],\n",
      "        [-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885],\n",
      "        [ 0.5976,  0.7885],\n",
      "        [ 0.5976,  0.7885]], grad_fn=<ViewBackward0>) torch.Size([9, 2])\n",
      "========\n",
      "tensor([[-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885],\n",
      "        [-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885],\n",
      "        [-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885]], grad_fn=<RepeatBackward0>) torch.Size([9, 2])\n",
      "========\n",
      "tensor([[-0.2666, -0.1805, -0.2666, -0.1805],\n",
      "        [-0.2666, -0.1805,  0.9001, -0.7813],\n",
      "        [-0.2666, -0.1805,  0.5976,  0.7885],\n",
      "        [ 0.9001, -0.7813, -0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813,  0.9001, -0.7813],\n",
      "        [ 0.9001, -0.7813,  0.5976,  0.7885],\n",
      "        [ 0.5976,  0.7885, -0.2666, -0.1805],\n",
      "        [ 0.5976,  0.7885,  0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885,  0.5976,  0.7885]], grad_fn=<CatBackward0>) torch.Size([9, 4])\n",
      "========\n",
      "tensor([[[-0.2666, -0.1805, -0.2666, -0.1805],\n",
      "         [-0.2666, -0.1805,  0.9001, -0.7813],\n",
      "         [-0.2666, -0.1805,  0.5976,  0.7885]],\n",
      "\n",
      "        [[ 0.9001, -0.7813, -0.2666, -0.1805],\n",
      "         [ 0.9001, -0.7813,  0.9001, -0.7813],\n",
      "         [ 0.9001, -0.7813,  0.5976,  0.7885]],\n",
      "\n",
      "        [[ 0.5976,  0.7885, -0.2666, -0.1805],\n",
      "         [ 0.5976,  0.7885,  0.9001, -0.7813],\n",
      "         [ 0.5976,  0.7885,  0.5976,  0.7885]]], grad_fn=<ViewBackward0>) torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Demonstration how above is derrived....\n",
    "print(h, \n",
    "      h.size())\n",
    "print(\"========\")\n",
    "print(h.repeat(1, N), \n",
    "      h.repeat(1, N).size()) # N = 3\n",
    "print(\"========\")\n",
    "print(h.repeat(1, N).view(N * N, -1), \n",
    "      h.repeat(1, N).view(N * N, -1).size())\n",
    "print(\"========\")\n",
    "print(h.repeat(N, 1), \n",
    "      h.repeat(N, 1).size())\n",
    "print(\"========\")\n",
    "print(torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1), \n",
    "      torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).size())\n",
    "print(\"========\")\n",
    "print(torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features), \n",
    "      torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ef24d-039d-40b7-b3f2-462d03e538ab",
   "metadata": {},
   "source": [
    "![title](./Images/a_input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d832e5-e608-44ff-9dcd-cbc7c1ab8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa78e50a-d3ca-47cc-82bb-bc2ea1cc8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Understanding the how thing are built...\n",
    "print(a_input.shape, a.shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input, a).shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input, a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9096d25-8d20-49a8-993f-bd174c25e6e9",
   "metadata": {},
   "source": [
    "## Mask Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c8b553-156c-4ef8-9f34-2d0196afb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "adj = torch.randint(2, (3, 3))\n",
    "\n",
    "zero_vec = -9e15 * torch.ones_like(e)\n",
    "print(zero_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c426426d-3147-452e-bb5d-8c1596435b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]]) \n",
      " \n",
      " tensor([[ 0.0413,  0.3608,  0.5604],\n",
      "        [-0.4482, -0.3843, -0.3444],\n",
      "        [-0.0239,  0.1999,  0.3995]], grad_fn=<LeakyReluBackward0>) \n",
      " \n",
      " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]]) \n",
      " \n",
      "\n",
      "attention:  tensor([[ 4.1342e-02, -9.0000e+15,  5.6045e-01],\n",
      "        [-4.4824e-01, -3.8435e-01, -9.0000e+15],\n",
      "        [-2.3920e-02, -9.0000e+15, -9.0000e+15]], grad_fn=<SWhereBackward0>) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "print(adj, \"\\n \\n\", e, \"\\n \\n\", zero_vec, \"\\n \\n\")\n",
    "# print(np.abs(attention.detach().numpy()))\n",
    "print(\"attention: \", attention, attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba3ca2d-e15b-4b97-8090-8512eb366819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3731, 0.0000, 0.6269],\n",
      "        [0.4840, 0.5160, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying softmax function\n",
    "attention = F.softmax(attention, dim=1)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f37f5b3-63c6-4fbd-a3a6-33f540e28a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3731, 0.0000, 0.6269],\n",
      "        [0.4840, 0.5160, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "tensor([[ 0.2752,  0.4270],\n",
      "        [ 0.3353, -0.4905],\n",
      "        [-0.2666, -0.1805]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_prime = torch.matmul(attention, h)\n",
    "\n",
    "print(attention)\n",
    "print(\"\")\n",
    "print(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07225508-af02-4094-ac8f-2b18e3cd0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2752,  0.4270],\n",
      "        [ 0.3353, -0.4905],\n",
      "        [-0.2666, -0.1805]], grad_fn=<MmBackward0>) \n",
      " tensor([[-0.2666, -0.1805],\n",
      "        [ 0.9001, -0.7813],\n",
      "        [ 0.5976,  0.7885]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# h_prime Vs h\n",
    "print(h_prime, \"\\n\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991d5ec-f03b-4eb4-892c-c4f6951ff7b6",
   "metadata": {},
   "source": [
    "## Build the GAT Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c765c7-dbac-4b70-81ac-0b37f8a30da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout # drop prob = 0.6\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha # LeakyReLU with negative slope, alpha = 0.2\n",
    "        self.concat = concat # concat = True for all the layers except the output layer\n",
    "        \n",
    "        # Xavier Initialization of Weights\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W) # matrix multiplication\n",
    "        N = h.size()[0]\n",
    "        print(N)\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        print(a_input.shape, e.shape)\n",
    "        \n",
    "        # Masked Attetion\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        \n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fdb83-8b5b-4693-940c-0a826da007d8",
   "metadata": {},
   "source": [
    "### Using the built layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b482e809-cb66-4547-acb2-1eb096e35774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in Cora: 7\n",
      "Number of Node Features in Cora: 1433\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "# from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_name = 'Cora'\n",
    "dataset = Planetoid(root='./data' + dataset_name, name=dataset_name)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {dataset_name}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {dataset_name}:\", dataset.num_node_features)\n",
    "# print(dataset[0])\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1d006d9-2537-4cbc-a62f-eac46f1b03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = GATLayer(dataset.num_node_features, 16, dropout=0.6, alpha=0.2)\n",
    "        self.gat2 = GATLayer(16, dataset.num_classes, dropout=0.6, alpha=0.2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ac3a88-defe-431c-8848-5024a7bafabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info: \n",
      "GAT(\n",
      "  (gat1): GATLayer(\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat2): GATLayer(\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n",
      "2708\n",
      "torch.Size([2708, 2708, 32]) torch.Size([2708, 2708])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10556) must match the size of tensor b (2708) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10664/2368672846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10664/1052972669.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10664/2808353745.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Masked Attetion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mzero_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9e15\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10556) must match the size of tensor b (2708) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = GAT()\n",
    "print(\"Model info: \")\n",
    "print(model)\n",
    "data = dataset[0]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c3adc-5597-410b-9628-ef24a72bd804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
