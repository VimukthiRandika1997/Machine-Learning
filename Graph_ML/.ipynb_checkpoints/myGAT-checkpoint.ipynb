{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62723d35-7d83-4d54-abe1-34344926841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def glorot(value: Any):\n",
    "    if isinstance(value, Tensor):\n",
    "        stdv = math.sqrt(6.0 / (value.size(-2) + value.size(-1)))\n",
    "        value.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    else: \n",
    "        for v in value.parameters() if hasattr(value, 'parameters') else[]:\n",
    "            glorot(v)\n",
    "        for v in value.buffers() if hasattr(value, 'buffers') else []:\n",
    "            glorot(v)\n",
    "            \n",
    "def constant(value: Any, fill_value: float):\n",
    "    if isinstance(value, Tensor):\n",
    "        value.data.fill_(fill_value)\n",
    "    \n",
    "    else: \n",
    "        for v in value.parameters() if hasattr(value, 'parameters') else[]:\n",
    "            constant(v, fill_value)\n",
    "        for v in value.buffers() if hasattr(value, 'buffers') else []:\n",
    "            constant(v, fill_value)\n",
    "            \n",
    "def zeros(value: Any):\n",
    "    constant(value, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "881b4ea6-c8ef-4be7-81ea-85b03220295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vimukthi/MyProjects/Machine-Learning/Graph_ML/Classes\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import NoneType\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "\n",
    "# from inits import glorot, zeros\n",
    "\n",
    "class GATConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample\n",
    "        out_channels (int): Size of each output sample\n",
    "        heads (int, optional): Number of multi-head atttention\n",
    "        concat (bool, optional): If set to 'False', the multi-head\n",
    "            attentions are average instead of concatenated\n",
    "            (default: 'True')\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope (default: '0.2')\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to to a stochastically\n",
    "            sampled neighbourhood during training (default: '0')\n",
    "        add_self_loops (bool, optional): If set to 'False', will not add\n",
    "            self loops to the input graph. (default: 'True')\n",
    "        edge_dim (int, optional): Edge feature dimensionality (if there any), default('None')\n",
    "        fill_value (float or Tensor or str, optional): The way to generate \n",
    "            edge features of self-loops (in case 'edge_dim' != None).\n",
    "        bias (bool, optional): If set to 'False', the layer wil not learn\n",
    "            an additive bias (default: 'True')\n",
    "        **kwargs (optional): Additional arguements of\n",
    "            :class 'torch_geometric.nn.conv.MessgePassing'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                in_channels: Union[int, Tuple[int, int]],\n",
    "                out_channels: int,\n",
    "                heads: int = 1,\n",
    "                concat: bool = True,\n",
    "                negative_slope: float = 0.2,\n",
    "                dropout: float = 0.0,\n",
    "                add_self_loops: bool = True,\n",
    "                edge_dim: Optional[int] = None,\n",
    "                fill_value: Union[float, Tensor, int] = 'mean',\n",
    "                bias: bool = True,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.edge_dim = edge_dim,\n",
    "        self.fill_value = fill_value\n",
    "        \n",
    "        # Bipartite graphs -> seperate transformations 'lin_src' and 'lin_dst' to source  and target nodes:\n",
    "        if isinstance(in_channels, int):\n",
    "            self.lin_src = Linear(in_channels, heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.lin_dst = self.lin_src\n",
    "            \n",
    "        else:\n",
    "            self.lin_src = Linear(in_channels[0], heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.lin_dst = Linear(in_channels[1], heads * out_channels, bais=False, weight_initializer='glorot')\n",
    "            \n",
    "        # Learnable parameters to compute attention coefficients\n",
    "        self.att_src = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_dst = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        \n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.att_edge = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        else:\n",
    "            self.lin_edge = None\n",
    "            self.register_parameter('att_edge', None)\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.lin_src.reset_parameters()\n",
    "        self.lin_dst.reset_parameters()\n",
    "        \n",
    "        glorot(self.att_src)\n",
    "        glorot(self.att_dst)\n",
    "        glorot(self.att_edge)\n",
    "        zeros(self.bias)\n",
    "        \n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], \n",
    "                edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, \n",
    "                size: Size = None,\n",
    "                return_attention_weights=None\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool): if set to True,\n",
    "            will return the tuple (edge_index, attention weights),\n",
    "            holding the weights for each edge. (default: None)\n",
    "        \"\"\"\n",
    "         \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        # Transform the input node features\n",
    "        # If a tuple is passed, transform source and target node features via seperate weights\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2, \"Static graphs not support in GATConv\"\n",
    "            x_src = x_dst = self.lin_src(x).view(-1, H, C)\n",
    "        else: # Tuple of source and targe node features\n",
    "            x_src, x_dst = x\n",
    "            assert x_src.dim() == 2, \"Static graphs not supported in GATConv\"\n",
    "            x_src = self.lin_src\n",
    "            if x_dst is not None:\n",
    "                x_dst = self.lin_dst(x_dst).view(-1, H, C)\n",
    "                \n",
    "        x = (x_src, x_dst)\n",
    "        \n",
    "        # Compute node-level attention coefficients,\n",
    "        # both for source and target nodes \n",
    "        alpha_src = (x_src * self.att_src).sum(dim=-1)\n",
    "        alpha_dst = None if x_dst is None else (x_dst * self.att_dst).sum(dim=-1)\n",
    "        alpha = (alpha_src, alpha_dst)\n",
    "        \n",
    "        \n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                num_nodes = x_src.size(0)\n",
    "                if x_dst is not None:\n",
    "                    num_nodes = min(num_nodes, x_dst.size(0))\n",
    "                num_nodes = min(size) if size is not None else num_nodes\n",
    "                edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "                edge_index, edge_attr = add_self_loops(edge_index, edge_attr, \n",
    "                                                       fill_value = self.fill_value, \n",
    "                                                       num_nodes=num_nodes)\n",
    "                \n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = set_diag(edge_index)\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    \"The usage of 'edge_attr' and 'add_self_loops'\"\n",
    "                    \"simultaneously is currently not yet supported for\"\n",
    "                    \"'edge_index' in a 'SparseTensor' form\"\n",
    "                )\n",
    "        # edge updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\n",
    "        alpha = self.edge_updater(edge_index, alpha=alpha, edge_attr=edge_attr)\n",
    "        \n",
    "        # propagate_type: (x: OptPairTensor, alpha: Tensor)\n",
    "        out = self.propagate(edge_index, x=x, alpha=alpha, size=size)\n",
    "        \n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # Along the second dimension\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "            \n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "    \n",
    "    def edge_update(self, alpha_j: Tensor, alpha_i: OptTensor,\n",
    "                    edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
    "                    size_i: Optional[int]) -> Tensor:\n",
    "        # Given edge-level attention coefficients for source and target nodes,\n",
    "        # we simply need to sum them up to \"emulate\" concatenation:\n",
    "        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i\n",
    "\n",
    "        if edge_attr is not None and self.lin_edge is not None:\n",
    "            if edge_attr.dim() == 1:\n",
    "                edge_attr = edge_attr.view(-1, 1)\n",
    "            edge_attr = self.lin_edge(edge_attr)\n",
    "            edge_attr = edge_attr.view(-1, self.heads, self.out_channels)\n",
    "            alpha_edge = (edge_attr * self.att_edge).sum(dim=-1)\n",
    "            alpha = alpha + alpha_edge\n",
    "\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return alpha\n",
    "    \n",
    "    # Compute the messages for all nodes using alpha weights\n",
    "    def message(self, x_j: Tensor, alpha: Tensor) -> Tensor:\n",
    "        return alpha.unsqueeze(-1) * x_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
