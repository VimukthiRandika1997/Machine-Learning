{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa9a032-1070-4f4d-a643-df2244e8a62a",
   "metadata": {},
   "source": [
    "# Import necessary Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26ad250-d3ca-4825-b697-3cc845c28eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import NoneType\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bc1137-cc99-4476-9e70-e0168e50cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def glorot(value: Any):\n",
    "    if isinstance(value, Tensor):\n",
    "        stdv = math.sqrt(6.0 / (value.size(-2) + value.size(-1)))\n",
    "        value.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    else: \n",
    "        for v in value.parameters() if hasattr(value, 'parameters') else[]:\n",
    "            glorot(v)\n",
    "        for v in value.buffers() if hasattr(value, 'buffers') else []:\n",
    "            glorot(v)\n",
    "            \n",
    "def constant(value: Any, fill_value: float):\n",
    "    if isinstance(value, Tensor):\n",
    "        value.data.fill_(fill_value)\n",
    "    \n",
    "    else: \n",
    "        for v in value.parameters() if hasattr(value, 'parameters') else[]:\n",
    "            constant(v, fill_value)\n",
    "        for v in value.buffers() if hasattr(value, 'buffers') else []:\n",
    "            constant(v, fill_value)\n",
    "            \n",
    "def zeros(value: Any):\n",
    "    constant(value, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8751d17-1acf-428a-a6c5-bbe4975643c6",
   "metadata": {},
   "source": [
    "# Creating the Edge-featured Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840115b7-50a7-4728-8158-99d8d16d98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFGAL(MessagePassing):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample\n",
    "        \n",
    "        out_channels (int): Size of each output sample\n",
    "        \n",
    "        heads (int, optional): Number of multi-head atttention\n",
    "        \n",
    "        concat (bool, optional):    If set to 'False', the multi-head\n",
    "                                    attentions are average instead of concatenated (default: 'True')\n",
    "            \n",
    "        negative_slope (float, optional):   LeakyReLU angle of the negative\n",
    "                                            slope (default: '0.2')\n",
    "                                            \n",
    "        dropout (float, optional):  Dropout probability of the normalized\n",
    "                                    attention coefficients which exposes each node to to a stochastically\n",
    "                                    sampled neighbourhood during training (default: '0')\n",
    "            \n",
    "        add_self_loops (bool, optional):    If set to 'False', will not add\n",
    "                                            self loops to the input graph. (default: 'True')\n",
    "            \n",
    "        edge_dim (int, optional): Edge feature dimensionality (if there any), default('None')\n",
    "        \n",
    "        fill_value (float or Tensor or str, optional):  The way to generate \n",
    "                                                        edge features of self-loops (in case 'edge_dim' != None).\n",
    "            \n",
    "        bias (bool, optional):  If set to 'False', the layer wil not learn\n",
    "                                an additive bias (default: 'True')\n",
    "            \n",
    "        **kwargs (optional):    Additional arguements of\n",
    "                                :class 'torch_geometric.nn.conv.MessgePassing'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                in_channels: Union[int, Tuple[int, int]],\n",
    "                out_channels: int,\n",
    "                heads: int = 1,\n",
    "                concat: bool = True,\n",
    "                negative_slope: float = 0.2,\n",
    "                dropout: float = 0.0,\n",
    "                add_self_loops: bool = True,\n",
    "                edge_dim: Optional[int] = None,\n",
    "                fill_value: Union[float, Tensor, int] = 'mean',\n",
    "                bias: bool = True,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.edge_dim = edge_dim,\n",
    "        self.fill_value = fill_value\n",
    "        \n",
    "        # Bipartite graphs -> seperate transformations 'lin_src' and 'lin_dst' to source  and target nodes:\n",
    "        if isinstance(in_channels, int):\n",
    "            self.lin_src = Linear(in_channels, heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.lin_dst = self.lin_src\n",
    "            \n",
    "        else:\n",
    "            self.lin_src = Linear(in_channels[0], heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.lin_dst = Linear(in_channels[1], heads * out_channels, bais=False, weight_initializer='glorot')\n",
    "            \n",
    "        # Learnable parameters to compute attention coefficients\n",
    "        self.att_src = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_dst = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        \n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False, weight_initializer='glorot')\n",
    "            self.att_edge = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        else:\n",
    "            self.lin_edge = None\n",
    "            self.register_parameter('att_edge', None)\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
