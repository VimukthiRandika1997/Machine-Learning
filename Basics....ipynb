{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec685b1-dfea-4b59-8946-1439a26fbce8",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- tokenization is the step of breaking down a string into atomic units used in the model\n",
    "- There're several tokenization strategies...\n",
    "- Optimal splitting of words into subunits is usuallay learned from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c07ee-ef78-429a-995f-4c435860bd68",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "\n",
    "- Simplest tokenization scheme, feeding each character individually to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495e3649-d1cf-40ad-9bc2-0722c08b049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 'k', 's', ' ', 'o', 'f', ' ', 'N', 'L', 'P']\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core taks of NLP'\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efd3a9-b700-429b-a7d3-003656483be1",
   "metadata": {},
   "source": [
    "Models expect each character to be converted to an integer, this process called *Numericalization*\n",
    "\n",
    "- One simple way to do this is by encoding each unqiue token with a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aab2da6-8db8-4ee1-b994-30be4b8e8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'L': 1, 'N': 2, 'P': 3, 'T': 4, 'a': 5, 'c': 6, 'e': 7, 'f': 8, 'g': 9, 'i': 10, 'k': 11, 'n': 12, 'o': 13, 'r': 14, 's': 15, 't': 16, 'x': 17, 'z': 18}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2708ebaa-a2e3-4bef-807e-f8f3f5454b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 13, 11, 7, 12, 10, 18, 10, 12, 9, 0, 16, 7, 17, 16, 0, 10, 15, 0, 5, 0, 6, 13, 14, 7, 0, 16, 5, 11, 15, 0, 13, 8, 0, 2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Transform tokenized_text according above mapping\n",
    "\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931ae3f-4f8e-48e6-a6c0-a0da01508e05",
   "metadata": {},
   "source": [
    "Last step is to convert input_ids to a 2D tensor of one-hot vectors.\n",
    "\n",
    "- **one hot** vectors are frequently used in ML to encode categorical data, which can be either ordinal or nomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741f753a-93c4-415a-af0a-1810fc9deee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example\n",
    "categorical_df = pd.DataFrame({'Name': ['Bumblebee', 'Optimus Prime', 'Megatron'], 'Label ID': [0, 1, 2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433463-64f2-4817-9ed4-388cbf6498ff",
   "metadata": {},
   "source": [
    "The problem of this approach is this creates an order between the names, and models can learn this kind of relationships.\n",
    "\n",
    "- Hence, we can create new columns for each category and assing 1 where the category is true, and a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd20482-0101-4ac3-a3d9-a5a9797cec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas we can use get_dummies()\n",
    "pd.get_dummies(categorical_df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e270f814-5304-4279-9e32-53250e6778e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1024124/2673273441.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids) # converts to a tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 19])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids) # converts to a tensor\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8238a6-0e02-4fae-a5e7-1471ec37b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 4\n",
      "One-hot: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# verifying the idea\n",
    "print(f'Token: {tokenized_text[0]}')\n",
    "print(f'Tensor index: {input_ids[0]}')\n",
    "print(f'One-hot: {one_hot_encodings[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67935a-a9bc-4ced-8962-a8a0e86fb63c",
   "metadata": {},
   "source": [
    "Character-level tokenization ignores any structure in the text an treats the whole string as stream of characters.\n",
    "- This helps to deal with misspelling and rare words\n",
    "- Drawback: requires significant compute as linguistics structurs (such as words) need to be learned from the data.\n",
    "\n",
    "Instead of this, we need to preserve some structure of the text is preserved during the tokenization step.\n",
    "- Hence Word Tokenization is being used most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a774d8-cc12-4772-8a13-862f85b3ad71",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "- Split the text into words and map each word to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a3cb19-315a-48ea-afe4-429f958ce3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'taks', 'of', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# using whitespace to tokenize the text\n",
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a519b0a-5b27-45dd-b7dd-81f5afeed9e8",
   "metadata": {},
   "source": [
    "some word tokenizers have extra rules for punctuation.\n",
    "- one can apply stemming or lemmatization, which normalize words to their stem, at the expense of losing some information of text\n",
    "- Ex: `greater`, `great` become `great`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac139f1c-6db7-4e9b-853f-3ae62491b1a5",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fc6e7-9a47-4afe-9cfc-67e24194bb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
